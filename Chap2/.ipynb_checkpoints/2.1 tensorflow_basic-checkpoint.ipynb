{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.layers.Dense\n",
    "    \n",
    "Dense란 신경망 구조의 가장 기본적인 형태를 의미.  \n",
    "즉, 아래의 수식을 만족하는 기본적인 신경망 형태의 층을 만드는 함수\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y=f(Wx+b)}\n",
    "\\end{align}\n",
    "\n",
    "위 수식에서 x는 입력 벡터, b는 편향 벡터, W는 가중치 행렬, f는 활성화 함수이다.  \n",
    "Dense 층을 구성하는 기본적인 방법은 가중치인 W와 b를 가각ㄱ 변수로 선언한 후 행렬 곱을 통해 구하는 방법.  \n",
    "다음과 같이 직접 가중치 변수를 모두 정의해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W = tf.Variable(tf.random.uniform([5, 10], -1.0, 1.0))  \n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y = tf.matmul(W, x) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 하나하나 선언하여 직접 곱하고, 더해야 하나 **Dense**를 이용하면 한 줄로 위의 코드 작성 가능  \n",
    "keras의 Dense를 사용하려면 우선 객체를 생성해야 함.\n",
    "\n",
    "dense = tf.kieras.layers.Dense( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 생성한 Dense 층 객체에 입력값을 넣어야 함.  \n",
    "입력값을 넣기 위해서는 객체를 생성할 때 함께 넣거나 생성한 후 따로 적용하는 방법이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 객체 생성 후 다시 호출하면서 입력값 설정  \n",
    "dense = tf.keras.layers.Dense( ... )  \n",
    "output = dense(input)\n",
    "\n",
    "\n",
    "#### 2. 객체 생성 시 입력값 설정  \n",
    "output = tf.keras.layers.Dense( ... )(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense 층을 만들 때 여러 인자를 통해 가중치와 편향 초기화 방법, 활성화 함수의 종류 등 여러 가지를 옵션으로 정할 수 있음.  \n",
    "객체를 생성할 때 지정할 수 있는 인자는 다음과 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__init__(  \n",
    "    units,  \n",
    "    activation = None,  \n",
    "    use_bias = True,   \n",
    "    kernel_initializer = 'glorot_uniform',  \n",
    "    bias_initializer = 'zeros',  \n",
    "    kernel_regularizer = None,  \n",
    "    bias_regularizer = None,   \n",
    "    activity_regularizer = None,  \n",
    "    kernel_constraint = None,  \n",
    "    bias_constraint = None,  \n",
    "    **kwargs  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 인자값이 의미하는 바가 무엇인지 하나씩 알아보자.\n",
    "\n",
    "* units : 출력 값의 크기. Integer 혹은 Long  \n",
    "* activation : 활성화 함수  \n",
    "* use_bias : 편향(b)을 사용할지 여부. Boolean 값 형태.  \n",
    "* kernel_initializer : 가중치(W) 초기화 함수  \n",
    "* bias_initializer : 편향 초기화 함수  \n",
    "* kernel_regularizer : 가중치 정규화 방법  \n",
    "* bias_regularizer : 편향 정규화 방법  \n",
    "* activity_regularizer : 출력 값 정규화 방법  \n",
    "* kernel_constraint : Optimizer에 의해 업데이트 된 이후에 가중치에 적용되는 부가적인 제약 함수(예 : norm constraint, value constriant)  \n",
    "* bias_constraint : Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수(예 : norm constraint, value constraint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제  \n",
    "입력값에 대해 활성화 함수로는 sigmoid 함수, 출력값은 10개의 값을 출력하는 완전 연결 계층(Fully Connected Layer) 정의해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10개의 노드를 가지는 은닉층이 있고 최종 출력 값은 2개의 노드가 있는 신경망 구조를 생각해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(inputs)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.layers.Dropout\n",
    "\n",
    "신경망 모델을 만들 때 생기는 여러 문제점 중 대표적인 문제점은 바로 **Overfitting**  \n",
    "이는 정규화(Regularization) 방법을 통해 해결하는데, 그중 가장 대표적인 방법이 **dropout**  \n",
    "\n",
    "텐서플로는 드롭아웃을 쉽게 모델에 적용할 수 있게 간단한 모듈을 제공하는데, 이 모듈을 이용하면 특정 keras.layers의 입력값에 드롭아웃을 적용할 수 있다.  \n",
    "사용법은 dense 층 만드는 방법과 유사하게 Dropout 객체를 생성해서 사용하면 된다.\n",
    "\n",
    "tf.keras.layers.Dropout( ... )\n",
    "\n",
    "#### 1. 객체 생성 후 다시 호출하면서 입력값 설정  \n",
    "dropout = tf.keras.layers.Dropout( ... )\n",
    "output = dropout(input)\n",
    "\n",
    "#### 2. 객체 생성 시 입력값 설정\n",
    "output = tf.keras.layers.Dropout( ... )(input)\n",
    "\n",
    "드롭아웃의 인자들을 살펴보자.\n",
    "\n",
    "init(\n",
    "rate,  \n",
    "noise_shape = None,  \n",
    "seed = Nont,  \n",
    "**kwargs,  \n",
    ")\n",
    "\n",
    "* rate : 드롭아웃을 적용할 확률. 확률 값이므로 0 ~ 1 사이의 값이며 예를 들어 dropout = 0.2면 전체 입력값 중에서 20%를 0으로 만든다.\n",
    "* noise_shape : 정수형의 1D-tensor 값을 받음. 여기서 받은 값은 shape을 뜻하는데, 이 값을 지정하여 특정 값만 드롭아웃을 적용할 수 있음. 예를 들어, 입력값이 이미지일 때 noise_shape을 지정하면 특정 채널에서만 드롭아웃이 가능.\n",
    "* seed : 드롭아웃의 경우 지정된 확률 값을 바탕으로 무작위로 드롭아웃을 적용하는데, 이때 임의의 선택을 위한 시드 값을 의미. seed 값은 정수형이며, 같은 seed 값을 가지는 드롭아우스이 경우 동일한 드롭아웃 결과를 가짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (20, 1)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.5)(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로에서 드롭아웃은 tf.keras.layers 뿐만 아니라 tf.nn 모듈에도 있는데, 두 모듈의 차이점은  \n",
    "tf.keras.layers.dropout의 경우 확률을 0.2로 지정했을 때 노드의 20%를 0으로 만드는 데 비해  \n",
    "tf.nn.dropout의 경우 확률을 0.2로 지정했을 때 80%를 0으로 만든다는 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_29:0\", shape=(None, 1, 28, 28), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE)\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)\n",
    "hidden = tf.keras.layers.Dense(units = 10, activation = tf.nn.sigmoid)(dropout)\n",
    "output = tf.keras.layers.Dense(units = 2, activation = tf.nn.sigmoid)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 처럼 드롭아웃을 적용하려는 층의 노드를 객체에 적용하면 됨.  \n",
    "위 예제는 입력값에 드롭아웃을 적용한 후 Dense 층 지나도록 설정한 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.layers.Conv1D  \n",
    "\n",
    "합성곱(Convolution) 연산 중 Conv1D에 대해 알아보자.  \n",
    "텐서플로의 합성곱 연산은 Conv1D, Conv2D, Conv3D로 나눠지는데 우선 이 세 개가 어떤 차이점이 있는지 알아보자.  \n",
    "\n",
    "우리가 흔히 알고 있는 기본적인 이미지에 적용하는 합성곱 방식은 Conv2D.  \n",
    "일반적으로 두 가지 기준으로 구분 가능하다.  \n",
    "1. 합성곱이 진행되는 방향  \n",
    "2. 합성곱 결과로 나오는 출력값\n",
    "\n",
    "* Conv1D\n",
    "  - 합성곱의 방향 : 한 방향(가로)\n",
    "  - 출력값 : 1-D Array(vector)\n",
    "* Conv2D\n",
    "  - 합성곱의 방향 : 두 방향(가로, 세로)\n",
    "  - 출력값 : 2-D Array(matrix)\n",
    "* Conv3D\n",
    "  - 합성곱의 방향 : 세 방향(가로, 세로, 높이)\n",
    "  - 출력값 : 3-D Array(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 크기와 합성곱이 적용되는 필터의 개수도 고려해야 하므로 출력값이 위와 동일하게 나오지는 않는다.  \n",
    "위의 경우는 단순히 배치의 경우는 고려하지 않고 합성곱 필터를 하나만 적용했을 때라고 생각하면 됨.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자연어 처리 분야에서 사용하는 합성곱의 경우 각 단어(혹은 문자) 벡터의 차원 전체에 대해 필터를 적용시키기 위해 주로 **Conv1D**를 사용.  \n",
    "\n",
    "그렇다면 Conv1D를 사용하는 방법을 알아보자\n",
    "\n",
    "#### 1. 객체 생성 후 다시 호출하면서 입력값 설정  \n",
    "conv1d = tf.keras.layers.Conv1D( ... )  \n",
    "output = conv1d(input)\n",
    "\n",
    "#### 2. 객체 생성 시 입력값 설정\n",
    "output = tf.keras.layers.Conv1D( ... )(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱도 피렅의 크기, 필터의 개수, 스트라이드 값 등을 객체 생성 시 인자로 설정할 수 있다.  \n",
    "인자 값에 따라 학습 성능이 크게 달라지므로 어떤 선택 사항이 있는지와 각 인자가 의미하는 바에 대해 정확하게 알고 있는 것이 중요.  \n",
    "\n",
    "__init__(  \n",
    "filters,  \n",
    "kernel_size,  \n",
    "strides = 1,  \n",
    "padding = 'valid',  \n",
    "data_format = 'chanels_last',  \n",
    "dilation_rate = 1,  \n",
    "activation = None,  \n",
    "use_bias = True,  \n",
    "kernel_initializer = 'glorot_uniform',  \n",
    "bias_initializer = 'zeros',  \n",
    "kernel_regularizer = None,  \n",
    "bias_regularizer = None,  \n",
    "activity_regularizer = None,  \n",
    "kernel_constraint = None,  \n",
    "bias_constraint = None,  \n",
    "**kwargs)\n",
    "\n",
    "\n",
    "구조는 몇 가지 제외하고 이전에 알아본 Dense와 비슷하다.  \n",
    "다른점은 합성곱 연산 수행 할 필터와 관련된 부분!  \n",
    "또한 합성곱은 기본적으로 필터의 크기를 필요로 하는데, 이 경우 Conv1D는 필터의 높이(high)는 필요하지 않음.  \n",
    "Conv1D의 필터는 입력값의 차원 수와 높이가 동일하게 연산되기 때문에 필터의 가로 길이만 설정하면 됨.  \n",
    "즉, 필터의 가로에 적용되는 kernel_size만 설정하면 됨.  \n",
    "그리고 총 몇 개의 필터를 사용할지 filters  인자를 통해 정해야 함.  \n",
    "또한 패딩을 사용해 입력값과 출력값의 가로 크기를 똑같이 하고 싶으면 padding = \"same\"을 지정하면 입력과 출력의 가로 길이가 같아짐.  \n",
    "그 외 다양한 옵션을 위한 인자를 알아보자.  \n",
    "\n",
    "* filters : 필터의 개수, 정수형. 출력의 차원 수를 의미\n",
    "* kernel_size : 필터의 크기로서, 정수 혹은 정수의 리스트. 튜플 형태로 지정. 합성곱이 적용되는 윈도우(window)의 길이를 나타냄. \n",
    "* strides : 적용할 스트라이드의 값으로서 정수 혹은 정수의 리스트, 튜플 형태로 지정. 1이 아닌 값을 지정할 경우 dilation_rate는 1 외외의 값을 지정하지 못함. \n",
    "* padding : 패딩 방법. \"VALIE\" 또는 \"SAME\"\n",
    "* data_format : 데이터의 표현 방법 선택. \"channel_last\"혹은 \"channel_first\"를 지정할 수 있음. channel_last의 경우 데이터는 batch, length, channels)형태여야 하고, channel_first의 경우 데이터는 (batch, channels, length) 형태.\n",
    "* dilation_rate : dilation 합성곱 사용 시 적용할 dilation 값으로서, 정수 혹은 정수의 리스트, 튜플 형태로 지정. 1이 아닌 값을 지정하면 strides 값으로 1 이외의 값을 지정하지 못함. \n",
    "* activation : 활성화 함수\n",
    "* use_bias : 편향(b)을 사용할지 여부. Boolean 값 형태\n",
    "* kernel_initializer : 가중치(W) 초기화 함수\n",
    "* bias_initializer : 편향 초기화 함수\n",
    "* kernel_regularizer : 가중치 정규화 방법\n",
    "* bias_regularizer : 편향 정규화 방법\n",
    "* activity_regularizer : 출력 값 정규화 방법\n",
    "* kernel_constraint : Optimizer에 의해 업데이트된 이후에 가중치에 적용되는 부가적인 제약 함수(예: norm constraint, value constraint)\n",
    "* bias_constraint : Optimizer에 의해 업데이트 된 이후에 편향에 적용되는 부가적인 제약 함수(예 : norm constraint, value constraint)\n",
    "\n",
    "Conv1D의 기본적인 사용법을 알아보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE[1:])\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape = INPUT_SIZE[1:])\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "         filters=10,\n",
    "         kernel_size=3,\n",
    "         padding='same',\n",
    "         activation=tf.nn.relu)(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.keras.layers.MaxPool1D\n",
    "\n",
    "합성곱 신경망과 함께 쓰이는 기법 중 하나는 풀링.  \n",
    "feature map의 크기를 줄이거나 주요한 특징을 뽑아내기 위해 합성곱 이후에 적용되는 기법.  \n",
    "\n",
    "풀링네는 주로 두가지 풀링 기법이 사용되는데, 이는 맥스 풀링(max-pooling)과 평균 풀링(average-pooling)이 있음.  \n",
    "- max-pooling : feature map에서 최대값만을 뽑아내는 방식\n",
    "- average-pooling : feature map에 대해 전체 값들을 평균한 값을 뽑는 방식\n",
    "\n",
    "맥스 풀링도 합성곱과 같이 세 가지 형태로 모델이 구분돼 있음. (MaxPool1D, MaxPool2D, MaxPool3D) -> 합성곱과 똑같은 원리  \n",
    "자연어 처리에 주로 사용되는 것은 합성곱과 동일하게 MaxPool1D -> 한 방향으로만 풀링이 진행.  \n",
    "\n",
    "#### 1. 객체 생성 후 apply 함수를 이용해 입력값 설정\n",
    "max_pool = tf.keras.layers.MaxPool1D( ... )  \n",
    "max_pool.apply(input)  \n",
    "\n",
    "#### 2. 객체 생성 시 입력값 설정\n",
    "max_pool = tf.keras.layers.MaxPool1D( ... )(input)\n",
    "\n",
    "__init__(  \n",
    "pool_size = 2,  \n",
    "strides = None,  \n",
    "padding = 'valid',  \n",
    "data_format = None,  \n",
    "**kwargs)\n",
    "\n",
    "* pool_size : 풀링을 적용할 필터의 크기. 정수값.\n",
    "* strides : 적용할 스트라이드 값. 정수 혹은 None 값.\n",
    "* padding : 패딩 방법. \"valid\" 또는 \"same\".\n",
    "* data_format : 데이터의 표현 방법. \"channel_last\" 혹은 \"channel_first\". channel_last의 경우(batch, length, channels) 형태, channel_first의 경우 데이터는 (batch, channels, length) 형태여야 함.\n",
    "\n",
    "예제)  \n",
    "입력값이 합성곱과 맥스 풀링을 사용한 후 완전 연결 계층을 통해 최종 출력 값이 나오는 구조 만들기. 그리고 입력값에는 드롭아웃 적용. 그리고 맥스 풀링 결과값을 완전 연결 계층으로 연결하기 위해서 행렬을 벡터로 만들어야함. (이때, tf.keras.layers.Flatten 사용.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = (1, 28, 28)\n",
    "\n",
    "inputs = tf.keras.Input(shape = INPUT_SIZE[1:])\n",
    "dropout = tf.keras.layers.Dropout(rate = 0.2)(inputs)\n",
    "conv = tf.keras.layers.Conv1D(\n",
    "          filters=10,\n",
    "          kernel_size=3,\n",
    "          padding='same',\n",
    "          activation=tf.nn.relu)(dropout)\n",
    "max_pool = tf.keras.layers.MaxPool1D(\n",
    "            pool_size = 3,\n",
    "            padding = 'same')(conv)\n",
    "flatten = tf.keras.layers.Flatten()(max_pool)\n",
    "hidden = tf.keras.layers.Dense(units = 50, activation = tf.nn.relu)(flatten)\n",
    "output = tf.keras.layers.Dense(units = 10, activation = tf.nn.softmax)(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2.0\n",
    "\n",
    "### API 정리 (API Cleanup)\n",
    "기존의 텐서플로 1.x 버전에서는 같은 기느을 수행하는 다양한 API가 다양한 패키지에 속해있었다.  \n",
    "그뿐만 아니라 복잡하게 흩어져 있던 다양한 API들을 모두 파악하고 사용하기가 매우 어려웠다.  \n",
    "텐서플로 2.0에서는 명료하고 사용하기 편하도록 동일한 기능의 다양한 API를 하나로 통합하고, 잘 사용하지 않는 다양한 API를 제거했다.  \n",
    "\n",
    "### 이거모드 (Eager execution)\n",
    "기존에는 텐서플로에서의 실행 방식은 우선 텐서플로 API를 이용해 그래프를 만든 후 별도 세션을 통해 해당 그래프를 실행하는 방식.  \n",
    "따라서 연산 그래프 만든 후 session.run()으로 추가 실행해야 값 확인 가능했음.  \n",
    "그러나 현재는 파이썬과 동일한 이거 모드로 실행되어 연산을 구성하면서 바로바로 값 확인 가능.  \n",
    "\n",
    "### 모델 구축\n",
    "텐서플로 2.0에서 keras를 활용해 모델을 구축하고 학습하는 것을 권장.  \n",
    "keras API는 고수준(high-level) API로서 사용하기 간편할 뿐더러 매우 유연하고 높은 성능을 보여줌.  \n",
    "keras를 활용한 모델 구축 방법은 크게 다음과 같다.  \n",
    "- Sequential API\n",
    "- Funtional API\n",
    "- Functional / Sequential API\n",
    "  - \\+ Custom Layers\n",
    "- Subclassing (Custom Model\n",
    "\n",
    "각 방법을 통해 모델 구축하는 과정을 하나씩 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential API\n",
    "tf.keras.Sequential이 keras를 활용해 모델을 구축할 수 있는 가장 간단한 형태의 API.  \n",
    "이 모듈을 이용하면 간단한 순차적인 Layer의 stack을 구현할 수 있음.  \n",
    "예를 들면 다음과 같은 방법으로 fully-connected layer 구현이 가능.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential 인스턴스 생성 후 해당 인스턴스에 여러 layer를 순차적으로 더하기만 하면 모델이 완성.  \n",
    "이렇게 만든 모델을 입력값을 더한 순서에 맞게 layer들을 통과시킨 후 최종 output을 뽑아오게 됨.  \n",
    "\n",
    "그에 반해 모델 구현에 제약이 있는데, 모델의 층들이 순차적으로 구성돼 있지 않은 경우에는 Sequential 모듈을 사용해 구현하기가 어려울 수 있음.  \n",
    "예) VQA(Visual Question Answering) 문제는 사진 데이터에서 특징을 뽑는 layer와 질문 텍스트 데이터에서 특징을 뽑는 두 layer가 각기 순차적으로 존재.  \n",
    "따라서 최종적으로 출력값을 뽑기 위해서 이 두 값을 합쳐야 하는데 이런 경우에는 Sequential 모듈로는 제약이 발생."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional API\n",
    "Sequential 모듈은 간단한 layer들의 스택 구조에는 적합하지만 복잡한 모델의 경우 여러 구현상의 제약이 잇을 수 있음.  \n",
    "예를 들어, 다음과 같은 경우 사용하기 어려움.  \n",
    "- 다중 입력값 모델(Multi-input models)\n",
    "- 다중 출력값 모델(Multi-output models)\n",
    "- 공유 층을 활용하는 모델(Models with shared layers)\n",
    "- 데이터 흐름이 순차적이지 않은 모델(Models with non-sequential data flows)\n",
    "\n",
    "이러한 모델 구현시 Functional API 또는 Subclassing 방식을 사용하는 것이 적절.  \n",
    "\n",
    "Functional API를 통해 위에서 정의한 모델과 동일한 모델을 만들어보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))\n",
    "x = layers.Dense(64, activation = 'relu')(inputs)\n",
    "x = layers.Dense(64, activation = 'relu')(x)\n",
    "predictions = layers.Dense(10, activation = 'softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Layer\n",
    "앞에서는 Sequential API와 Functional API를 사용하기 위해 keras의 layers 패키지에 정의된 레이어를 사용해 구현함.  \n",
    "대부분 구현하고자 하는 모듈의 경우 해당 패키지에 구현돼있지만 새로운 연산을 하는 레이어 혹으느 편의를 위해 여러 레이어를 하나로 묶은 레이어를 구현해야 하는 경우가 있음.  \n",
    "이때 사용자 정의 층(custom layer)을 만들어 사용하면 됨.  \n",
    "앞에서 정의한 모델에서는 dense 층이 여러 번 사용된 신경망을 사용.  \n",
    "이 신경망을 하나의 레이어로 묶어 재사용성을 높이고 싶으면 다음과 같은 새로운 사용자 정의 층으로 정의하면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer) :\n",
    "    \n",
    "    # 하이퍼파라미터가 객체 생성 시 호출되도록 __init__메서드에서 정의\n",
    "    def __init_(self, hidden_dimension, hidden_dimension2, output_dimension) :\n",
    "        self.hidden_dimension = hidden_dimension\n",
    "        self.hidden_dimension2 = hidden_dimension2\n",
    "        self.output_dimension = output_dimension\n",
    "        super(CustomLayer, self).__init__()\n",
    "    \n",
    "    # 모델의 가중치와 관련된 값은 build 메서드에서 생성되도록 정의\n",
    "    def build(self, input_shape) :\n",
    "        self.dense_layer1 = layers.Dense(self.hidden_dimension, activation = 'relu')\n",
    "        self.dense_layer2 = laeyrs.Dense(self.hidden_dimension2, activation = 'relu')\n",
    "        sekf,dense_laeyr3 = layers.Dense(self.output_dimension, activation = 'softmax')\n",
    "    \n",
    "    # call 메서드에서 정의한 값들을 이용해 해당 층의 logic을 정의.\n",
    "    def call(self, inputs) :\n",
    "        x = self.dense_layer1(inputs)\n",
    "        x = self.dense_layer2(x)\n",
    "        \n",
    "        return self.dense_layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(CustomLayer(64, 64, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subclassing (Custom Model)\n",
    "가장 자유도가 높은 모델.  \n",
    "tf.keras.Model을 상속받고 모델 내부 연산들을 직접 구현하는 모델.  \n",
    "모델 클래스를 구현할 때는 객체를 생성할 때 호출되는 __init__ 메서드와 생성된 인스턴스를 호출할 때 (즉, 모델 연산이 사용될 때) 호출되는 call 메서드만 구현하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model) :\n",
    "    \n",
    "    def __init__ (self, hidden_dimension, hidden_dimension2, output_dimension) :\n",
    "        super(MyModel, self).__init__(name = 'my model')\n",
    "        self.dense_layer1 = layers.Dense(hidden_dimension, activation = 'relu')\n",
    "        self.dense_layer2 = layers.Dense(hidden_dimension2, activation = 'relu')\n",
    "        self.dense_layer3 = layers.Dense(hidden_dimension3, activation = 'softmax')\n",
    "        \n",
    "    def call(self, inputs) :\n",
    "        x = self.dense_laeyr1(inputs)\n",
    "        x = self.dense_layer2(x)\n",
    "        \n",
    "        return self.dense_layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구현 방법은 Custom Layer를 만드는 방식과 매우 유사ㅏ.  \n",
    "__init__ 메서드에서 모델에 사용될 층과 변수를 정의하고, call 메서드에서는 정의한 내용을 활용해 모델 연산을 진행.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습\n",
    "tensorflow 2.0 공식 가이드에서 모델 학습에 대해 권장하는 방법은 크게 두 가지로 나뉨. \n",
    "\n",
    "1. 케라스 모델의 내장 API를 활용\n",
    "2. 학습, 검증, 예측 등 모든 과정을 GradientTape 객체를 활용해 직접 구현하는 방법\n",
    "\n",
    "첫 번째 방법의 경우 대부분 keras model의 메서드로 이미 구현돼 있어 간편.  \n",
    "두 번재 방법의 경우 첫 번째 방법과 비교했을 때 일일이 구현해야 한다는 단점이 있지만 좀 더 복잡한 로직을 유연하고 자유롭게 구현 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 내장 API를 활용하는 방법\n",
    "이미 정의 된 keras의 모델 객체가 있다고 가정해보자.  \n",
    "이 모델 객체는 keras의 모델 객체이기 때문에 여러 메서드가 이미 내장돼 있다.  \n",
    "따라서 내장 메서드를 간단히 사용만 하면 됨.  \n",
    "\n",
    "먼저 해야 할 일은 학습 과정을 정의 하는 것.  \n",
    "즉, 학습 과정에서 사용될 손실 함수(loss function), 옵티마이저(Optimizer), 평가에 사용될 지표(metric)등을 정의하면 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "             loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "             metrics = [tf.keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고로 위와 같이 객체 형식으로 지정해도 되지만 다음과 같이 문자열 형태로 지정해도 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의된 모델 객체를 대상으로 학습, 평가, 에측 메서드를 호출하면 정의한 값들을 활용해 학습이 진행됨.  \n",
    "즉, 다음과 같이 fit 메서드를 호출하면 데이터들이 모델을 통과하며 학습이 진행됨.  \n",
    "아울러 학습이 진행되면서 각 에폭 당 모델의 성능(손실함수, 정확도) 등이 출력되는 것을 확인할 수 있음.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size = 64,\n",
    "          epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 과정에서 각 epoch 마다 검증을 진행하는 것도 가능.  \n",
    "evaluate  메서드를 사용해 검증할 수 있지만 매번 epoch을 호출해야 한다는 번거로움이 있음.  \n",
    "따라서 epoch 마다 검증 겨로가를 보기 위해서는 fit 함수에 검증 데이터를 추가로 넣으면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size = 64,\n",
    "          epochs = 3,\n",
    "          validation_data = (x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
