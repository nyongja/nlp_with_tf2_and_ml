{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn (사이킷런)\n",
    "\n",
    "사이킷런은 파이썬용 머신러닝 라이브러리.  \n",
    "지도학습을 위한 모듈, 비지도 학습을 위한 모듈, 모델 선택 및 평가를 위한 모듈, 데이터 변환 및 데이터를 불러오기 위한 모듈, 계산 성능 향상을 위한 모듈로 구성돼 있음.  \n",
    "\n",
    "지도 학습 모듈 - 나이브 베이즈(Naive Bayes), 의사결정 트리(Descision Trees), 서포트 벡터 머신(Support Vector Machines) 모델  \n",
    "비지도 학습 모듈 - 군집화(Clustering), 가우시안 혼합 모델(Gaussian mixture models)  \n",
    "모델 선택, 평가 모듈 - 교차 검증(Cross validation), 모델 평가(Model evaluation)  \n",
    "데이터 변환 모듈 - 파이프라인(Pipeline), 특징 추출(Feature extraction), 데이터 전처리(preprocessing data), 차원 축소(dimensionality reduction)  \n",
    "등의 기능이 있음.  \n",
    "\n",
    "또한, 머신러닝 연구와 학습을 위해 라이브러리 안에 자체적으로 데이터 셋을 포함하고 있고, 쉽게 불러와서 사용이 가능함.  \n",
    "라이브러리에서 기본적으로 제공되는 데이터로는 당뇨병 데이터, 아이리스 데이터, 유방암 데이터 등이 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.22.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 붓꽃(Iris) 데이터\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_dataset key : dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "iris_dataset = load_iris()\n",
    "# 데이터가 어떻게 구성되어 있는지 살펴보기\n",
    "print(\"iris_dataset key : {}\".format(iris_dataset.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "shape of data : (150, 4)\n"
     ]
    }
   ],
   "source": [
    "# 키 값을 하나씩 뽑아서 확인해 보자.\n",
    "print(iris_dataset['data'])\n",
    "print(\"shape of data : {}\".format(iris_dataset['data'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data에는 실제 데이터가 포함돼 있음. 각 데이터마다 4개의 특징(feature)를 가지고 있다.  \n",
    "데이터의 형태를 보면 (150, 4)로 전체 150개의 데이터가 각각 4개의 특징값을 가지고 있는 형태.  \n",
    "4개의 특징값이 의미하는 바를 확인하기 위해 \"feature_names\"값을 확인해 보자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['feature_names'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "각각 꽃의 꽃받침 길이, 꽃받침 너비, 꽃잎 길이, 꽃잎 너비를 의미함.\n",
    "\n",
    "이번에는 'target'에 대해 알아보자.  \n",
    "'target'값와 'target_names'값을 함게 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['target'])\n",
    "print(iris_dataset['target_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'target'은 각 데이터에 대한 정답, 즉 라벨값을 가지고 있다.  \n",
    "결과를 보면 해당 데이터는 총 3개의 라벨로, 'target_naems'값을 살펴보면 각각 Setosa, Versicolor, Virginica라는 붓꽃의 세 가지 종을 각각 0 1 2로 나타냄을 확인 가능하다.  \n",
    "\n",
    "마지막으로 'DESCR'에 대해 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 'Description'의 약자로서 해당 데이터에 대한 전체적인 요약 정보를 보여준다.  \n",
    "\n",
    "이제 이 데이터를 사용해 3개의 타깃으로 분류하는 모델을 만들어 보자.  \n",
    "정확한 성능 평가를 위한 평가 데이터를 만들기 위해 150개의 학습 데이터 중 일부를 평가를 위한 데이터로 분리해보자.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런을 이용한 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_label, test_label = train_test_split(iris_dataset['data'], iris_dataset['target'], test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split 함수에 우선 나누고 싶은 데이터를 넣는다.  \n",
    "여기서는 데이터 값과 라벨인 타겟 값을 넣는다.  \n",
    "그리고 나서 평가 데이터의 크기를 결정해야 한다.  \n",
    "이 값의 경우 0과 1 사이의 비율을 입력해야 함 -> 0.25는 25%의 데이터를 평가 데이터로 분리한다는 읨.  \n",
    "마지막으로 random_state값응ㄹ 석ㄹ정하는데 이는 함수가 데이터를 나눌 때 무작위로 데이터를 선택해서 나누는데, 이를 제어할 수 있는 값이다.  \n",
    "함수를 여러번 사용해도 이를 똑같이 설정할 경우 동일한 데이터를 선택해서 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train_input : (112, 4)\n",
      "shape of test_input : (38, 4)\n",
      "shape of train_label : (112,)\n",
      "shape of test_label : (38,)\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train_input : {}\".format(train_input.shape))\n",
    "print(\"shape of test_input : {}\".format(test_input.shape))\n",
    "print(\"shape of train_label : {}\".format(train_label.shape))\n",
    "print(\"shape of test_label : {}\".format(test_label.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터가 존재하지 않는 경우 이처럼 학습 데이터의 일부분을 평가 데이터로 사용.  \n",
    "평가 데이터가 있어도 학습 데이터의 일부분을 따로 분리하는 경우가 있는데, 이는 학습 데이터를 학습 데이터와 검증 데이터로 구분하기 위함.  \n",
    "즉, 학습 데이터, 평가 데이터, 검증 데이터 총 3개 데이터로 나뉘는 경우인데,   \n",
    "이는 학습 데이터를 사용해서 모델을 학습시키고 학습시킨 모델에 대해 일차적으로 검증 데이터를 사용해 모델 검증을 진행한 후, 그 결과를 통해 모델의 하이퍼파라미터를 수정한다.  \n",
    "이처럼 학습과 검증 과정을 반복적으로 진행한 후 최종적으로 나온 모델에 대해 평가 데이터를 사용해 평가한다.   \n",
    "\n",
    "이제 붓꽃 데이터를 대상으로 사이킷런을 통해 모델을 만드는 방법을 알아보자. \n",
    "\n",
    "\n",
    "### 사이킷런을 이용한 지도 학습\n",
    "**지도학습(Supervised Learning)이란?**  \n",
    "각 데이터에 대해 정답이 있는 경우 각 데이터의 정답을 예측할 수 있게 학습시키는 과정.  \n",
    "즉, 모델이 에측하느 ㄴ결과를 각 데이터의 정답과 비교해서 모델을 반복적으로 학습. \n",
    "\n",
    "**k최근접 이웃 분류기(K-nearest neighbor classifier)**  \n",
    "예측하고자 하는 데이터에 대해 가장 가까운 거리에 있는 데이터의 라벨과 같다고 예측하는 방법.  \n",
    "데이터에 대한 사전 지식이 없는 경우의 분류에 많이 사용.  \n",
    "k값은 예측하고자 하는 데이터와 가까운 몇 개의 데이터를 참고할 것인지를 의미.  \n",
    "\n",
    "**특징**\n",
    "* 데이터에 대한 가정이 없어 단순 \n",
    "* 다목적 분류와 회귀에 좋음\n",
    "* 높은 메모리 요구\n",
    "* k값이 커지면 계산이 늦어짐\n",
    "* 관련 없는 기능의 데이터의 규모에 민감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류기 생성 시 인자 값으로 n_neighbors를 받는데, 이는 위에서 말한 k값을 의미.  \n",
    "즉, 여기서는 k = 1인 분류기를 생성한 것.  \n",
    "이렇게 생성한 분류기를 학습 데이터에 적용하면 됨.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(train_input, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric = 'minkowski', \n",
    "                    metric_params = None, n_jobs = 1, n_neighbors = 1, p = 2,\n",
    "                    weights = 'uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 fit함수를 사용해 분류기 모델에 학습 데이터와 라벨 값을 적용하기만 하면 모델 학습이 간단하게 끝남!  \n",
    "\n",
    "이제 학습시킨 모델을 사용해 새로운 데이터의 라벨을 예측해보자.  \n",
    "우선은 새롭게 4개의 feature값을 임의로 설정해서 numpy 배열로 만들자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_input = np.array([[6.1, 2.8, 4.7, 1.2]]) # 참고로 배열 생성 시 리스트 안에 하나의 리스트가 포함된 방식으로 만들어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict(new_input) # 1(Versicolor)로 예측함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 모델 성능을 측정하기 위해 이전에 따로 분리해둔 평가 데이터를 사용해 모델의 성능을 측정해보자.  \n",
    "우선 따로 분리해둔 평가 데이터에 대해 예측을 하고 그 결괏값을 변수에 저장하자.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "predict_label = knn.predict(test_input)\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 예측한 결괏값과 실제 결괏값을 비교해서 정확도가 어느 정도인지 측정해보자.  \n",
    "실제 겨로가와 동일한 것의 개수 평균을 구하면 됨. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 1.00\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도가 1.00으로 매우 좋은 성능을 보여줌.  \n",
    "이것은 데이터 자체가 특징이 라벨에 따라 구분이 잘 되고 모델이 데이터에 매우 적합함을 의미함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런을 이용한 비지도 학습\n",
    "\n",
    "**비지도학습(Unsupervised Learning)이란?**  \n",
    "비지도학습이란 정답, 즉 라벨을 사용하지 않고 만들 수 잇는 모델.  \n",
    "모델을 통해 문제를 해결하고 싶은데 데이터에 대한 정답이 없는 경우에 적용하기 적합한 모델.  \n",
    "\n",
    "**k평균 군집화(K-means Clustering) 모델**  \n",
    "군집화(Clustering)이란 데이터를 특성에 따라 여러 집단으로 나누는 방법.  \n",
    "따라서 붓꽃 데이터의 경우네느 3개의 정답이 있으므로 3개의 군집으로 나누는 방법을 사용해야 함.  \n",
    "\n",
    "우선 k개만큼의 중심을 임의로 설정하고, 모든 데이터를 가장 가까운 중심에 할당하며, 같은 중심에 할당 된 데이터들을 하나의 군집으로 판단.  \n",
    "각 군집 내 데이터들을 가지고 군집의 중심을 새로 구해서 업데이트.  \n",
    "이후 또 다시 가까운 중심에 할당되고 이러한 과정이 계속 반복.  \n",
    "이러한 반복은 하당되는 데이터에 변화가 없을 때까지 이뤄짐.  \n",
    "이후 반복이 종료되면 각 데이터가 마지막으로 할당된 중심에 따라 군집이 나뉨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=3, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.fit(train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전과 같이 fit함수를 사용해 데이터와 라벨을 입력하면 자동으로 데이터를 군집화한다.  \n",
    "단, supervised learning과의 차이점은 fit함수의 인자로 데이터의 라벨값을 넣지 않는다는 것!  \n",
    "\n",
    "라벨값을 넣지 않고 단순히 데이터 사이의 거리를 이용해 군집화한 것이기 때문에 바로 붓꽃의 라벨을 예측할 수는 없음.  \n",
    "하지만 다음과 같이 군집화한 k-평균 군집화 모델의 라벨 속성을 확인하면 각 데이터의 라벨을 확인할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1,\n",
       "       1, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 2, 2, 0, 1, 1, 0, 2, 1, 1, 1, 0,\n",
       "       2, 1, 2, 2, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       2, 2, 1, 0, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_means.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 붓꽃의 라벨이 아니라 3개(n_clusters = 3)의 군집을 의미함.  \n",
    "즉, k_means.labels에 나온 0이라는 라벨은 0번째 군집을 의미함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 clusters :  [2 1 1 1 2 1 1 1 1 1 2 1 1 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 1 1 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 2 2 1 2 1]\n",
      "1 clusters :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "2 clusters :  [2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"0 clusters : \", train_label[k_means.labels_ == 0])\n",
    "print(\"1 clusters : \", train_label[k_means.labels_ == 1])\n",
    "print(\"2 clusters : \", train_label[k_means.labels_ == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 중요한 것은 항상 결과가 위와 동일하게 나오지 않고 매번 바뀜.  \n",
    "즉 0번째 군집에 라벨 0인 데이터들이 주로 분포할 수도 있음.  \n",
    "이는 k-평균 군집화 모델 알고리즘 특성 때문에 불가피한 현상.  \n",
    "k-평균 군집화의 경우 처음 초기값을 랜덤으로 설정한 후 군집화 과정을 진행하기 때문.  \n",
    "\n",
    "이번에는 임의의 새로운 데이터를 만들어서 예측해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = np.array([[6.1, 2.8, 4.7, 1.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "prediction = k_means.predict(new_input)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 평가 데이터를 적용해 성능을 측정해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 0 0 1 0 2 0 0 2 1 1 1 1 0 2 0 0 2 1 0 1 2 2 2 2 2 1 1 1 1 0 1 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "predict_cluster = k_means.predict(test_input)\n",
    "print(predict_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 데이터를 적용시켜 예측한 군집을 이제 각 붓꽃의 종을 의미하는 라벨값으로 다시 바꿔줘야 실제 라벨과 비교해서 성능 측정이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array(predict_cluster)\n",
    "np_arr[np_arr == 0], np_arr[np_arr == 1], np_arr[np_arr == 2] = 3, 4, 5\n",
    "np_arr[np_arr == 3] = 1\n",
    "np_arr[np_arr == 4] = 0\n",
    "np_arr[np_arr == 5] = 2\n",
    "predict_label = np_arr.tolist()\n",
    "print(predict_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 라벨과 비교해서 성능이 어느 정도 되는지 확인해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "print('test accuracy {:.2f}'.format(np.mean(predict_label == test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 supervised 모델보다는 낮은 성능이나 그래도 매우 높은 편.  \n",
    "데이터의 라벨 없이 이정도의 성능이면 매우 우수한 결과.  \n",
    "\n",
    "따라서 만약 데이터는 있지만 라벨이 없는 경우 unsupervised learning 모델을 쓰는 것도 나쁘지 않음.  \n",
    "\n",
    "만약 데이터가 수치형 데이터가 아닌 경우에는 어떻게 할 수 있을까?  \n",
    "예를 들어, 인간 언어의 경우 수치화돼 있지 않아 다양한 머신러닝에 바로 적용 불가!  \n",
    "\n",
    "따라서 이어지는 절에서는 이러한 특징 추출 모듈에 대해 알아보겠다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런을 이용한 특징 추출\n",
    "\n",
    "NLP에서 특징 추출이란? 텍스트 데이터 -> 단어나 문장들을 어떤 특징 값으로 바꿔주는 것.  \n",
    "기존에 문자로 구성돼 있던 데이터를 모델에 적용할 수 있게 특징을 뽑아 어던 값으로 바꿔서 수치화 하는 것.  \n",
    "\n",
    "텍스트 데이터를 수치화하는 세 가지 방법에 대해 알아보자.  \n",
    "세 가지 모두 텍스트를 벡터로 만드는 방법.  \n",
    "- CountVectorizer : 각 텍스트에서 횟수를 기준으로 특징 추출\n",
    "- TfidfVectorizer : TF-IDF라는 값을 사용해 텍스트에서 특징 추출\n",
    "- HashingVectorizer : CountVectorizer와 동일하나 텍스트 처리 시 해시 함수를 사용해 실행 시간을 크게 줄이는 방법. 텍스트의 크기가 클 수록 효율적임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer\n",
    "텍스트 데이터에서 횟수를 기준으로 특징을 추출하는 방법.  \n",
    "어떤 단위의 횟수를 셀 것인지는 선택 사항. (e.g. 단어, 문자 하나하나 등)  \n",
    "보통은 단어를 기준으로 횟수를 측정하여, 문장 입력시 단어의 횟수를 측정한 뒤 벡터로 만듦.  \n",
    "\n",
    "먼저, 객제를 만들어야 함. 그리고 이 객체에 특정 텍스트를 적합시켜야 함.  \n",
    "이때 적합이란? 횟수를 셀 단어의 목록을 만드는 과정.  \n",
    "그 다음에 횟수를 기준으로 해당 텍스트를 벡터화.  \n",
    "\n",
    "예를 들어, \"나는 매일 공부를 한다\" 라는 문장을 횟수값으로 이뤄진 벡터로 만든다면, 우선 단어 사전을 정의해야 함.  \n",
    "이때 단어 사전이 \"나는\", \"너가\", \"매일\", \"공부를\", \"한다\", \"좋아한다\" 라는 6개의 단어로 구성돼 있다고 한다면  \n",
    "\"나는 매일 공부를 한다\"의 경우 [1, 0, 1, 1, 1, 0]이라는 벡터로 바뀔 것.  \n",
    "만약, \"나는 매일매일 공부를 한다.\"라는 문장에 대해서는 [1, 0, 2, 1, 1, 0]이라는 벡터로 바뀔 것.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 텍스트를 불러오자.  \n",
    "여기저는 직접 정의해서 사용해보겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부해야지']\n",
    "\n",
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 사전을 만들어보자.  \n",
    "생성한 객체에 fit 함수를 사용해 데이터를 적용하면 자동으로 단어 사전을 생성한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 3, '배가': 7, '고프다': 0, '내일': 4, '점심': 8, '뭐먹지': 6, '공부': 1, '해야겠다': 9, '먹고': 5, '공부해야지': 2}\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer.fit(text_data)\n",
    "print(count_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 텍스트 데이터를 실제로 벡터로 만들어보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sentence = [text_data[0]] # ['나는 배가 고프다']\n",
    "print(count_vectorizer.transform(sentence).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "횟수를 사용해서 만들기 때문에 직관적이고 간단함.  \n",
    "그러나 단순히 횟수만을 특징으로 잡아 큰 의미는 없음.  \n",
    "또한 자주 사용되는 단어들, 예를 들면 조사 혹은 지시대명사가 높은 특징 값을 가지기 때문에 유의미하게 사용하기 어려울 수도 있음.  \n",
    "따라서 이러한 문제를 해결할 수 있는 TF-IDF 방식의 특징 추출 방법을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "TF-IDF 특정한 값을 사용해서 텍스트 데이터의 특징을 추출하는 방법.  \n",
    "**TF(Term Frequency)** 란 특정 당어가 하나의 데이터 안에서 등장하는 횟수를 의미함.  \n",
    "**DF(Document Frequency)** 는 문서 빈도 값으로, 특정 단어가 여러 데이터에 자주 등장하는지를 알려주는 지표.  \n",
    "**IDF(Inverse Document Frequency)** 는 이 값에 역수를 취해 구한 값으로 특정 단어가 다른 데이터에 등장하지 않을수록 값이 커짐을 의미.  \n",
    "**TF-IDF** 란 이 두 값을 곱해서 사용하므로 어떤 단어가 해당 문서에 자주 등장하지만 다른 문서에는 많이 없는 단어일수록 높은 값.  \n",
    "따라서 조사나 지시대명사처럼 자주 등장하는 단어는 TF 값은 크나 IDF값은 작아져 앞선 문제를 해결할 수 있다.  \n",
    "\n",
    "사용방법은 CountVectorizer와 거의 유사.  \n",
    "결괏값만 단어의 출현 횟수가 아닌 각 단어의 TF-IDF 값으로 나오는 것만 다르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = ['나는 배가 고프다', '내일 점심 뭐먹지', '내일 공부 해야겠다', '점심 먹고 공부 해야지']\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 2, '배가': 6, '고프다': 0, '내일': 3, '점심': 7, '뭐먹지': 5, '공부': 1, '해야겠다': 8, '먹고': 4, '해야지': 9}\n",
      "['점심 먹고 공부 해야지']\n",
      "[[0.57735027 0.         0.57735027 0.         0.         0.\n",
      "  0.57735027 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.52640543 0.         0.66767854\n",
      "  0.         0.52640543 0.         0.        ]\n",
      " [0.         0.52640543 0.         0.52640543 0.         0.\n",
      "  0.         0.         0.66767854 0.        ]\n",
      " [0.         0.43779123 0.         0.         0.55528266 0.\n",
      "  0.         0.43779123 0.         0.55528266]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer.fit(text_data) # 단어 사전 만들기\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "\n",
    "sentence = [text_data[3]] # ['점심 먹고 공부 해야지']\n",
    "print(tfidf_vectorizer.transform(text_data).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 특징 추출 방법으로 TF-IDF 값을 사용하면 단순 횟수를 이용하는 것보다 각 단어의 특성을 좀 더 잘 반영할 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
