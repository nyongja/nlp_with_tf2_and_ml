{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 자연어 처리 개요\n",
    "\n",
    "자연어 처리의 경우 어떤 문제를 해결하려고 하느냐에 따라 분류되는데 이번 챕터에서는 총 4가지 핵심 문제, *텍스트 분류, 텍스트 유사도, 텍스트 생성, 기계 이해*에 대해 알아보자.\n",
    "\n",
    "그 전에 먼저 **단어 표현** 이라는 분야에 대하 먼저 알아보자.  \n",
    "단어 표현은 모든 자연어 처리 무ㅜ제의 기본 바탕이 되는 개념으로 자연어를 어떻게 표현할지 정하는 것이다.  \n",
    "\n",
    "자연어 처리를 포함한 모든 데이터 과학 분야에서는 데이터를 이해하는 것이 매우 중요.  \n",
    "단순히 데이터를 사용하는 것보다 데이터가 어떤 구조이고, 어떤 특성이 있는지 파악한 후 모델을 만드는 것이 훨씬 좋은 성과를 보여주기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 단어 표현\n",
    "자연어 처리는 컴퓨터가 인간의 언어를 이해하고 분석 가능한 모든 분야를 말함.  \n",
    "따라서 자연어 처리의 가장 기본적인 문제는 **'어떻게 자연어를 컴퓨터에게 인식시킬 수 있을까?'**  \n",
    "\n",
    "컴퓨터는 단어를 0과 1로 인식.  \n",
    "그러나 이러한 수치는 언어적인 특성이 전혀 반영이 되어 있지 않아 자연어 처리를 위해 만드는 모델에 적용하기 어렵다!  \n",
    "따라서 언어적인 특성을 반영해서 단어를 수치화 하는 방법이 바로 **단어 표현**  \n",
    "\n",
    "이렇게 단어를 수치화 할 때는 주로 벡터로 표현.  \n",
    "따라서 단어 표현은 \"단어 임베딩(word embedding)\" 또는 \"단어 벡터(word vector)\"로 표현하기도 함.  \n",
    "단어 표현에는 다양한 방법이 있고, 계속해서 연구되는 분야이므로 하나의 정답이 있는 것은 아님!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**원-핫 인코딩(one-hot encoding)** : 단어를 하나의 벡터로 표현하는 방법으로, 0과 1 값만 가지는 벡터로 표현.  \n",
    "이름에서 알 수 있는 벡터 값 가운데 하나만 1, 나머지는 모두 0으로 여기서 1이 되는 것이 각 단어가 어떤 단어인지 알려주는 인덱스.  \n",
    "이는 매우 간단하나 결정적인 두 가지 문제점이 있다.  \n",
    "1) 총 단어의 수만큼의 크기가 필요해 공간을 많이 사용하고, 큰 공간에 비해 실제 사용하는 값은 1이 되는 값 하나라 매우 비효율적.  \n",
    "2) 단순히 단어가 무엇인지만 알려주고, 벡터값 자체에는 단어의 의미나 특성 같은 것들이 전혀 표현되지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 문제점을 해결하기 위해 벡터의 크기가 작으면서도 벡터가 단어의 의미를 표현할 수 있는 방법이 제시 되었음.  \n",
    "이는 \"분포 가설(Distributed hypothesis)\"을 기반으로함.  \n",
    "\n",
    "**분포 가설** : 같은 문맥의 단어, 즉 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다.  \n",
    "\n",
    "1) 카운트 기반(count-base) : 특정 문맥 안에서 단어들이 동시에 등장하는 횟수를 직접 세는 방법  \n",
    "2) 예측(predict) : 신경망 등을 통해 문맥 안의 단어들을 예측하는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 카운트 기반 방법\n",
    "위에서 설명했듯이 카운트 기반 방법이란 어떤 글의 문맥 안에 단어가 동시에 등장하는 횟수를 세는 방법!  \n",
    "여기서 동시에 등장하는 횟수를 co-occurrence라고 함.  \n",
    "카운트 기반 방법은 기본적으로 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화해서 단어 벡터로 만드는 방법.  \n",
    "- 특이값 분해(Singular Value Decomposition, SVD)\n",
    "- 잠재의미분석(Latent Semantic Analysis, LSA)\n",
    "- Hyperspace Analogue to Language(HAL)\n",
    "- Hellinger PCA(Principal Component Analysis)\n",
    "\n",
    "위의 방법은 모두 co-occurrence Matrix를 만들고 그 행렬들을 변형하는 방식!  \n",
    "\n",
    "카운트 기반 방법은 빠르다는 장점이 있음.  \n",
    "예측 방법에 비해 좀 더 이전에 만들어진 방법이나 데이터가 많은 경우에는 단어가 잘 표현되고 효율적이어서 아직까지 많이 사용하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 방법\n",
    "예측 기반 방법 : 신경망 구조 혹은 어떠한 모델을 사용해 특정 문맥에서 어떤 단어가 나올지를 예측하면서 단어를 벡터로 만드는 방식.  \n",
    "예측방법에는 다음과 같은 것이 있다. \n",
    "- Wod2Vec\n",
    "- NNLM(Neural Network Language Model)\n",
    "- RNNLM(Recurrent Neural Network Language Model)\n",
    "\n",
    "이 중 가장 많이 사용되는 Word2Vec에 대해 자세히 알아보자.  \n",
    "Word2Vec에는 **CBOX(Continuous Bag of Words)** 와 **Skig-Gram** 이라는 두 가지 모델로 나뉜다.  \n",
    "- CBOW : 어떤 단어를 문맥 안의 주변 단어들을 통해 예측\n",
    "- Skip-Gram : 어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측\n",
    "\n",
    "#### CBOW 학습 순서\n",
    "1. 각 주변 단어들을 원-핫 벡터로 만들어 입력값으로 사용\n",
    "2. 가중치 행렬(weight matrix을 각 원-핫 벡터에 곱해 n-차원 벡터 만들기(n-차원 은닉층)\n",
    "3. 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다(출력층 벡터)\n",
    "4. n-차원 벡터에 다시 가중치 행렬을 곱해 원-핫 벡터와 같은 차원의 벡터로 만든다\n",
    "5. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교해서 학습한다\n",
    "\n",
    "#### Skip-Gram 학습 순서\n",
    "1. 하나의 단어를 원-핫 벡터로 만들어서 입력값으로 사용(입력층 벡터)\n",
    "2. 가중치 행렬을 원-핫 벡터에 곱해 n-차원 벡터를 만든다(n-차원 은닉층)\n",
    "3. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다(출력층 벡터)\n",
    "4. 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 원-핫 벡터와 비교해서 학습한다\n",
    "\n",
    "두 모델의 학습 과정이 비슷해 보이지만 확실한 차이점이 있음.  \n",
    "CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교함.  \n",
    "Skip-Gram에서는 입력값이 하나의 단어를 사용하고, 하긋ㅂ을 위해 주변의 여러 단어와 비교.  \n",
    "\n",
    "위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용.  \n",
    "\n",
    "기존의 카운트 기반 방법으로 만든 단어 벡터보다 단어 간의 유사도를 잘 측정함.  \n",
    "또 한가지 장점은 단어들의 복잡한 특징도 잘 잡아냄.  \n",
    "또한 서로에게 유의미한 관계를 측정할 수 있다는 점. \n",
    "\n",
    "CBOW보다는 Skip-Gram이 대체로 성능이 좋으나 절대적인건 아님!   \n",
    "\n",
    "**Glove** : 예측 + 카운트 기반 둘다 포함하는 단어 표현 방법!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
